<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Linear method for regression</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</head>

<body>
<hr/>

<p>layout: post
title: [notes][ESL] Chapter 3: Overview of Supervised Learning
comments: true
published: false</p>

<hr/>

<h1>Linear method for regression</h1>

<h1>3.1 Introduction</h1>

<p>Assumption: the regression function \( E[Y|X] \) is linear in the input vector \( X_1,...,X_p \).</p>

<p>Linear model is simple, but often provide <strong>adequate</strong> and <strong>interpretable</strong> description of how inputs affect the ouput.</p>

<p>For scenarios, where</p>

<ul>
<li>small number of training samples</li>
<li>low signal-to-noise ratio</li>
<li>sparse data</li>
</ul>

<p>linear methods can outperform fancier non-linear methods.</p>

<p>And we can this non-linear transformation, basis expansion or some other techniques to extend its scope of applications.</p>

<h1>3.2 Linear Regression Models and Least Squares</h1>

<p>The linear regression model has the form</p>

<p>\[ 
f(X) = \beta_0 + \sum_{j = 1}^p X_j\beta_j
 \]</p>

<p>Variables \( X_j \) come from various sources:</p>

<blockquote>
<ul>
<li>quantitative inputs;</li>
<li><strong>transformations</strong> of quantitative inputs, such as log, square-root or square</li>
<li><strong>basis expansions</strong>, such as X_2 = X_1<sup>2,</sup> X_3 = X_1<sup>3,</sup> leading to a polynomial representation;</li>
<li><strong>numeric or &ldquo;dummy&rdquo; coding</strong> of the levels of qualitative inputs. </li>
<li><strong>interactions</strong> between variables, for example, \( X_3 = X_1 X_2 \).</li>
</ul>
</blockquote>

<p>\( Least squares \) is a popular way of estimating \( \beta \). First the residual sum of squares is defined</p>

<p>\[ 
RSS(\beta) = \sum_{i = 1}^N(f(x_i) - y_i)
 \]</p>

<p>And we want the \( \beta \) that minimize \( RSS(\beta) \), i.e. \( \hat{\beta} = argmin_{\beta}RSS(\beta) \).</p>

<p>Note that <em>least squares</em> does not assume that our model is linear. It is a general way of estimation.</p>

<p>For linear models, we have </p>

<p>\[ 
\begin{equation}
  \begin{split}
    RSS(\beta) &= \sum_{i = 1}^N(f(x_i) - y_i)
    &= \sum_{i = 1}^N(\sum_{j}^px_{ij}\beta_j - y_i)
  \end{split}
\end{equation}
 \]</p>

<p>Assume that we have added a \( \mathbf{1} \) vector to the input vector, then we can rewrite the \( RSS(\beta) \) as </p>

<p>\[ 
RSS(\beta) = (\mathbf{y} - \mathbf{X}\beta)^T(\mathbf{y} - \mathbf{X}\beta)
 \]</p>

<p>Differentiate it w.r.t \( \beta \) and we get </p>

<p>\[ 
\frac{\partial RSS}{\partial \beta} = -2 \mathbf{X}^T(\mathbf{y} - \mathbf{X}\beta)
 \]</p>

<p>Assuming that \( X \) is full column rank, thus \( X^TX \) is positive definite and hence it is invertible, we set the derivate above to zero and get the <strong>unique</strong> solution</p>

<p>\[ 
\hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
 \]</p>

<p>Then the predicted values at the training samples are</p>

<p>\[ 
\begin{equation}
  \begin{split}
  \hat{\mathbf{y}} = \mathbf{X}\hat{\beta} = \mathbf{X}(\mathbf{X}^T\mathbf{X})\mathbf{X}^T\mathbf{y}
  \end{split}
\end{equation}
 \]</p>

<p>The matrix \( H = \mathbf{X}(\mathbf{X}^T\mathbf{X})\mathbf{X}^T \) is called &ldquo;hat&rdquo; matrix as it puts a hat on \( y \).</p>

<p>The geometrical explanation about how we estimate is as follows. Our target is to minimize \( RSS(\beta) = \| \mathbf{y} - \mathbf{\hat{y}}\|_2^2 \). Then \( \mathbf{y} - \mathbf{\hat{y}} \) is orthogonal to the column space of \( \mathbf{X} \).</p>

<p>\[ 
X^T(\hat{\mathbf{y}} - \mathbf{y}) = 0
 \]</p>

<p>Then</p>

<p>\[ 
X^T(X\beta - \mathbf{y}) = 0
 \]</p>

<p>What is interesting here is that it seems that we rediscover \( \frac{\partial RSS(\beta)}{\partial \beta} = 0 \) from a different point of view. Here \( H \) can be called <em>projection matrix</em>, as it projects \( \mathbf{y} \) on to the column space of \( \mathbf{X} \).</p>

<p>When \( \mathbf{X} \) contains linear dependent variables, \( X^TX \) is singular. But \( \hat{\mathbf{y}} \) are still projection of \( \mathbf{y} \) on the column space. The difference is that we have multiple(infinite?) representations of \( \hat{\mathbf{y}} \) now.</p>

<h2>Sampling properties of \( \hat{\beta} \)</h2>

<p>Assuming that \( y_i \) are uncorrelated and have constant \( \sigma^2 \). \( x_i \) are fixed(not random). Then as \( \hat{\beta} = (\mathbf{X}^T\mathbf{X})\mathbf{X}^T\mathbf{y} \), the variance-covariance matrix of the least squares estimates is</p>

<p>\[ 
Var(\hat{\beta}) = (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2
 \]</p>

<p>\( \sigma^2 \) can be estimated by</p>

<p>\[ 
\hat{\sigma^2} = \frac{1}{N-p-1} \sum_{i = 1}^N(y_i - \hat{y_i})^2,
 \]</p>

<p>which is an unbiased estimate of \( \sigma^2 \).</p>

<p>Now we assume that linear model is the right model for the regression function and the way \( Y \) is generated from \( X \) is</p>

<p>\[ 
\begin{equation}
  \begin{split}
  Y &= E[Y|X] + \epsilon\\
  &=X\beta + \epsilon,
  \end{split}
\end{equation}
 \]</p>

<p>where \( \epsilon \sim N(0, \sigma^2) \).</p>

<p>Then </p>

<p>\[ 
\begin{equation}
  \begin{split}
  E[\hat{\beta}] &= E[(\mathbf{X}^T\mathbf{X})\mathbf{X}^T\mathbf{y}] \\
  &=E[(\mathbf{X}^T\mathbf{X})\mathbf{X}^T(X\beta + \epsilon)]\\
  &=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\beta\\
  &=\mathbf{I}\beta = \beta
  \end{split}
\end{equation}
 \]</p>

<p>Thus we have </p>

<p>\[ 
\hat{\beta} \sim N(\beta, (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2)
 \]</p>

<p>Also,</p>

<p>\[ 
(N-p-1)\hat{\sigma}^2 =\sum_{i = 1}^N(y_i - \hat{y_i})^2 = \sigma^2\sum_{i = 1}^N(\frac{y_i - x_i^T\hat{\beta}}{\sigma})^2 \sim \sigma^2 \chi^2_{N-p-1}
 \]</p>

<p>Note that the two estimations \( \hat{sigma} \) and \( \hat{\beta} \) are statistically independent and thus we can construct the \( Z-score \) to test the hypothesis that a particular coefficient \( \beta_j = 0 \)</p>

<p>\[ 
z_j  = \frac{\hat{\beta}_j}{\hat{\sigma}\sqrt{(\mathbf{X}^T\mathbf{X})_{jj}^{-1}}} = \frac{\hat{\beta}_j}{\sqrt{(\mathbf{X}^T\mathbf{X})_{jj}^{-1}\sigma^2\frac{\hat{\sigma}^2}{(N-p-1)\sigma^2}}} = \frac{(\hat{\beta}_j)/(\sqrt{(\mathbf{X}^T\mathbf{X})_{jj}^{-1}\sigma^2})}{\sqrt{\frac{\hat{\sigma}^2}{(N-p-1)\sigma^2}}} 
 \]</p>

<p>Thus under the null hypothesis \( \beta_j = 0 \),</p>

<ul>
<li>\( z_j \) is distributed as \( t_{N - p - 1} \).(If it can be zero, we can exclude this feature) </li>
<li>A large value of \( \hat{\beta_j} \) would reject the null hypothesis. </li>
<li>If we use the true variance \( \sigma \) instead of the \( \hat{\sigma} \), this would turn into a standard normal distribution.</li>
<li>As \( N \) increases, the quantiles of a t-distribution approaches \( N(0, 1) \).</li>
</ul>

<p>To test the significance of a group of coefficients simultaneously, e.g. a group of dummy variables generated from a single categorical variable, we need to test whether it is ok to set the coefficients of all these features to zero. <em>Here we use F statistic</em>,</p>

<p>\[ 
F = \frac{(RSS_0 - RSS_1)/(p_1 - p_0)}{RSS_1/(N - p_1 - 1)}
 \]</p>

<p>where</p>

<ul>
<li>\( RSS_1 \): the residual sum of squares for the least squares fit with the tested features, \( RSS_0 \) for the same without tested features.</li>
<li>\( p_1 \): number of features with tested features, \( p_0 \) is the number without tested features.</li>
</ul>

<blockquote>
<p>The F statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an esti- mate of \( \sigma^2 \)</p>
</blockquote>

<p>With the Gaussian assumption and the null hypothesis that the model with less coefficients is correct,</p>

<ul>
<li>\( F \sim F_{p_1 - p_0, N-p_1-1} \).</li>
<li>For dropping single coefficient, \( z_j \) is equivalent to \( F \) statistic.</li>
<li>For large \( N \), the quantiles of \( F_{p_1-p_0, N-p_1 - 1} \) approaches \( \chi^2_{p_1 - p_0}/(p_1 - p_0) \).</li>
</ul>

<p><em>In summary, we use Z-score to see if a feature is significant and F-stat to see if a group of features are significant.</em></p>

<p><strong>Problem:</strong> If two features are very correlated, then inclusion of one will make the other not significant. As a result, we may exclude both of them if we purely follow the independently computed \( Z-score \).</p>

<h3>The Gauss-Markove Theorem</h3>

<p>The <em>least squares estimates</em> \( \hat{\beta} \) of \( \beta \) have the <em>smallest</em> variance among all <em>linear unbiased estimates</em>.</p>

<p>However, unbiased assumption may be unnecessary, we could sacrifice some bias for even lower variance, which results in a lower expected generalization error.</p>

<p>Now we try to prove that it is a unbised estimates and has the lowest variance</p>

<p><strong>Unbiased estimates</strong></p>

<p>Consider the case that we are estimating the linear combinations of the parameters \( \theta = \alpha^T\beta \), then the least squares estimates of \( \alpha^T\beta \) is</p>

<p>\[ 
\theta = \alpha \hat{\beta} = 
 \]</p>

<h2>3.3 Subset Selection</h2>

<p>Two problems with least squares estimates:</p>

<ul>
<li>Prediction accuracy: low bias, but high variance. We may trade some bias for less variance.</li>
<li>Interpretation: When faced with numerous features, we would like to see the &ldquo;big picture&rdquo; of what is important.</li>
</ul>

<p>In subset selection, we only keep a subset of the variables in the model. <em>Least squares regression</em> is used to estimate the coefficients of the remained inputs.</p>

<h3>3.3.1 Bes-subset selection</h3>

<blockquote>
<p>Best subset regression finds for each \( k \in \{0, 1, 2, . . . , p\} \) the subset of size \( k \) that gives smallest residual sum of squares.</p>
</blockquote>

<p>The problem of choosing \( k \) involves the tradeoff between bias and variance, and the way we do that is to measure the expected prediction error with the help of cross validation.</p>

<p>Some algorithms, such as leaps and bounds, can solve this problem up to \( p = 30 or 40 \).</p>

<h3>3.3.2 Forward and Backward-Stepwise Selection</h3>

<p>Instead of searching all possible subsets, we can seek a good path through them.</p>

<p><em>Forward and Backward-Stepwise Selection</em> </p>

</body>

</html>

