<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Bias-Variance</title>
        <meta name="viewport" content="width=device-width">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

        <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script> 

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-45812015-1', 'kinslover.github.io');
      ga('send', 'pageview');

    </script>            


    </head>
    <body>

        <div class="site">
          <div class="header">
            <h1 class="title"><a href="/">TK_blog</a></h1>
            <a class="extra" href="/">Home</a>
            <a class="extra" href="/stats">Stats</a>
            <a class="extra" href="/ml">Machine Learning</a>
          </div>

          <h2>Bias-Variance</h2>
<p class="meta">13 Nov 2013</p>

<div class="post">
<p>Bias-variance tradeoff is a very basic and important topic in machine learning. It basically tells us what makes up the generalization error of a hypothesis or even a leaner and the involved tradeoff between the bias and variance. By knowing this, we can better choose the learner and hypothesis that achieves low generalization errors.</p>

<h1 id="assumptions">1. Assumptions</h1>

<p>All examples are <script type="math/tex">i.i.d</script>, from the joint distribution <script type="math/tex">P(X, Y)</script>. In this article, uppercase letters represent the random variables, while lowercase letters represent constant values.</p>

<h1 id="three-levels">2. Three levels</h1>

<ul>
  <li><strong>Prediction</strong>: Given a dataset <script type="math/tex">d</script>, the hypothesis learned from this dataset, and the value of <script type="math/tex">x</script>. The prediction <script type="math/tex">h(x)</script> can be evaluated by the error . </li>
</ul>

<script type="math/tex; mode=display">
E_{Y}[err(h(x|d), Y)]
</script>

<ul>
  <li><strong>Hypothesis</strong>: A hypothesis maps <em>X</em> to <em>Y</em>. It can be evaluated by the quality of predictions. It means that we want to measure the expected error of the hypothesis learned from some training set <script type="math/tex">d</script> over the distribution <script type="math/tex">P(X, Y)</script>.</li>
</ul>

<script type="math/tex; mode=display">
E_{X,Y}[err(h(X|d), Y)]
</script>

<ul>
  <li><strong>Learner</strong>: A learner maps <em>dataset D</em> to <em>hypothesis h</em>. It can be evaluated by the quality of hypothesis. It means that we want to measure the expectation of the performance of the hypotheses over different training datasets <script type="math/tex">D</script>.</li>
</ul>

<script type="math/tex; mode=display">
E_{D}[E_{X,Y}[err(h(X|D), Y)]]
</script>

<h1 id="error-function">3. Error function</h1>

<p>So let us focus on the case with <script type="math/tex">L2</script> error function. </p>

<script type="math/tex; mode=display">\begin{equation}
	err(h(x|d), Y) = (h(x|d) - Y)^2
\end{equation}</script>

<h1 id="analysis-for-evaluation-of-predictions">4. Analysis for evaluation of predictions</h1>

<p>For a given <script type="math/tex">x</script> and <script type="math/tex">h(x)</script>, we measure our prediction over the distribution <script type="math/tex">P(Y\vert x)</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
	\begin{split}
		 E_{Y|x}[err(h(x), Y)] &= E_{Y|x}[(h(x) - Y)^2]\\
		&= E_{Y|x}[(h(x) -E[Y|x]+E[Y|x] - Y)^2]\\
		&= E_{Y|x}[(h(x) -E[Y|x])^2] + E_{Y|x}[(E[Y|x] - Y)^2] \\
		&+ 2 E_{Y|x}[(h(x)-E[Y|x])(E[Y|x] - Y)]\\
		&= (h(x) -E[Y|x])^2 + E_{Y|x}[(E[Y|x] - Y)^2] \\		
	\end{split}
\end{equation} %]]></script>

<ul>
  <li><strong>Fisrt term</strong>: the square of the bias</li>
  <li><strong>Second term</strong>: the variance of the variable <script type="math/tex">Y\vert x</script>, which is a property of the distribution and thus totally out of our control.</li>
</ul>

<p>And from the above derivation, we know that the hypothesis that minimizes the expected error of our prediction is</p>

<script type="math/tex; mode=display">
h^{opt}(x) = E[Y|x].
</script>

<h1 id="analysis-for-evaluation-of-hypotheses">5. Analysis for evaluation of hypotheses</h1>

<p>Given the training set <script type="math/tex">d</script>, we measure the hypothesis learned from this set with the expected error of its predictions over the distribution of all possible sample pair <script type="math/tex">P(X, Y)</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
	\begin{split}
		 E_{X, Y}[err(h(X|d), Y)] &= E_{X, Y}[(h(X|d) - Y)^2]\\
		&= E_{X}\{E_{Y|X}[(h(X|d) -E_{Y|X}[Y])^2] + E_{Y|X}[(E_{Y|X}[Y] - Y)^2]\}
	\end{split}
\end{equation} %]]></script>

<h1 id="analysis-for-evaluation-of-learner">6. Analysis for evaluation of learner</h1>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
	\begin{split}
		 E_{D}\{E_{X, Y}[err(h(X|D), Y)]\} &= E_{D}\{E_{X, Y}[(h(X|D) - Y)^2]\}\\
		 &= E_{D}\{E_{X,Y}[(h(X|D) -E_{Y|X}[Y])^2 + E_{X,Y}[(E_{Y|X}[Y] - Y)^2]\}\\
		&= E_{X,Y} \{ E_{D}[(h(X|D) -E_{D}[h(X|D)])^2 + E_{D}[(E_{D}[h(X|D)] - E_{Y|X}[Y])^2]] \\
		& + 2 E_{D}[(h(X|D) -E_{D}[h(X|D)])(E_{D}[h(X|D)] - E_{Y|X}[Y]) \} \\
		&+ E_{X,Y}[(E_{Y|X}[Y] - Y)^2]\\
		&= E_{X,Y} \{ E_{D}[(h(X|D) -E_{D}[h(X|D)])^2]\} \\&
		+ E_{X,Y} \{(E_{D}[h(X|D)] - E_{Y|X}[Y])^2 \}+ E_{X,Y}[(E_{Y|X}[Y] - Y)^2]\\		
	\end{split}
\end{equation} %]]></script>

<ul>
  <li>The first two terms make the <strong>approximation error</strong> together. 
    <ul>
      <li><strong>First term</strong>: the variance of our prediction over different training sets <script type="math/tex">D</script>. </li>
      <li><strong>Second term</strong>: the bias, i.e., how well our prediction approximates the true <script type="math/tex">Y</script>. </li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Third term: the expected variance of $$Y</td>
          <td>X<script type="math/tex"> over the distribution of </script>P(X)$$, i.e. the random noise.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h1 id="furthe-analysis">7. Furthe analysis</h1>

<p>Let </p>

<script type="math/tex; mode=display">
\hat{E}_{X, Y}[err(f(X), Y)] = \frac{1}{t} \sum^{t}_{i = 1} err(f(x_i), y_i),
</script>

<p>where <script type="math/tex">(x_i,y_i) \in D</script>, be the error of a certain hypothesis on training set.</p>

<p>Now suppose we have a hypothesis space <script type="math/tex">H</script>, <script type="math/tex">\hat{h}(X\vert D)</script> is the hypothesis <script type="math/tex">\in H</script> that minimizes the <script type="math/tex">MSE</script> on training dataset <script type="math/tex">D</script>, i.e.,</p>

<script type="math/tex; mode=display">
\hat{h}(X|D) = argmin_{h \in H} \hat{E}_{X,Y}[(h(X) - Y)^2]
</script>

<p>and <script type="math/tex">h^*(X)</script> is the hypothesis <script type="math/tex">\in H</script> that minimizes the expected <script type="math/tex">MSE</script> over <script type="math/tex">P(X, Y)</script>,i.e. </p>

<script type="math/tex; mode=display">
h^*(X) = argmin_{h \in H} E_{X,Y}[(h(X) - Y)^2]
</script>

<p>It is easy to see that </p>

<script type="math/tex; mode=display">
E_{D}[\hat{h}(X|D)] = h^*(X)
</script>

<p>Then we consider about the expected error of this learner</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
	\begin{split}
		 E_{D}\{E_{X, Y}[err(\hat{h}(X|D), Y)]\} &= E_{D}\{E_{X, Y}[(\hat{h}(X|d) - Y)^2]\}\\
		&= E_{X,Y} \{ E_{D}[(\hat{h}(X|D) - h^*(X))^2 + E_{D}[(h^*(X) - Y)^2]]\\
		& + 2 E_{D}[(\hat{h}(X|D) -h^*(X))](h(X) - Y) \}\\
		&= E_{X} \{ E_{D}[(\hat{h}(X|D) - h^*(X))^2]\} + E_{X,Y}\{(h^*(X) - Y)^2\}\\
	\end{split}
\end{equation} %]]></script>

<ul>
  <li><strong>First term</strong>: the variance of our learner, as <script type="math/tex">E_{D}[\hat{h}(X \vert D)] = h^*(X)</script>. </li>
  <li><strong>Second term</strong>: the combination of the bias and noise and also reppresent the lowest possible MSE on future sample, as <script type="math/tex">h^*(X) = argmin_{h \in H} E_{X,Y}[(h(X) - Y)^2]</script>.</li>
</ul>

<p>As <script type="math/tex">h_*(X)</script> is <strong>does not depend on training dataset D</strong>, we have </p>

<script type="math/tex; mode=display">
E_{X, Y}[(h^*(X) - Y)^2] = E_{D}\hat{E}_{X, Y}[(h^*(X) - Y)^2]
</script>

<p>which means that the training error of <script type="math/tex">h^*</script> is an <strong>unbiased estimate</strong> of its test error.</p>

<p>Then for <script type="math/tex">\hat{E}_{X, Y}[(h^*(X) - Y)^2]</script>, we have </p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
	\begin{split}
\hat{E}_{X, Y}[(h^*(X) - Y)^2] 
		&= \hat{E}_{X, Y}[(h^*(X) - \hat{h}(X|D) + \hat{h}(X|D) - Y)^2]\\
		&= \hat{E}_{X, Y}[(h^*(X) - \hat{h}(X|D))^2] + \hat{E}_{X, Y}[(\hat{h}(X|D) - Y)^2]\\
		& + 2 \hat{E}_{X, Y}[(h^*(X) - \hat{h}(X|D))(\hat{h}(X|D) - Y)]\\
		&= \hat{E}_{X, Y}[(h^*(X) - \hat{h}(X|D))^2] + \hat{E}_{X, Y}[(\hat{h}(X|D) - Y)^2]
	\end{split}
\end{equation} %]]></script>

<p>So we have </p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
	\begin{split}
		 E_{D}\{E_{X, Y}[err(\hat{h}(X|D), Y)]\} 
		 &=  E_{D}E_{X,Y}[(\hat{h}(X|D) - h^*(X))^2] + E_{X, Y}[(h^*(X) - Y)^2]\\
		 &= E_{D}E_{X,Y}[(\hat{h}(X|D) - h^*(X))^2] + E_{D}\hat{E}_{X, Y}[(h^*(X) - Y)^2]\\
		 &= E_{D} E_{X}[(\hat{h}(X|D) - h^*(X))^2] + E_{D}[\hat{E}_{X}[(\hat{h}(X|D)-h^*(X))^2]]\\ 
		 &+ E_{D}[\hat{E}_{X, Y}[(\hat{h}(X|D) - Y)^2]]\\
	\end{split}
\end{equation} %]]></script>

<ul>
  <li><strong>First term</strong>: variance of the leaner for future sample, as said above; </li>
  <li><strong>Second term</strong>: the variance of the learner on training dataset;</li>
  <li><strong>Third term</strong>: the leaner’s <script type="math/tex">MSE</script> on training set, i.e. the expected training error. So we can easily conclude that for a learner, <strong>the expected test error <script type="math/tex">\geq</script> optimal test error <script type="math/tex">\geq</script> expected training error</strong>.</li>
</ul>

<h1 id="the-tradeoff">8. The Tradeoff</h1>

<h2 id="increase-the-size-of-training-set-t--vert-d-vert">Increase the size of training set <script type="math/tex">t = \vert D \vert</script></h2>

<ul>
  <li>the optimal test error <script type="math/tex">E_{X, Y}[(h^*(X) - Y)^2]</script> does not change
    <ul>
      <li>the expected training error increases</li>
      <li>the training variance decreases</li>
    </ul>
  </li>
  <li>the variance on future samples decreases</li>
</ul>

<p>Thus the overall expected test error decreases. There is no tradeoff in this part.</p>

<h2 id="increase-the-repressiveness-of-the-hypothesis-space">Increase the repressiveness of the hypothesis space</h2>

<ul>
  <li>the optimal test error <script type="math/tex">E_{X, Y}[(h^*(X) - Y)^2]</script> decreases
    <ul>
      <li>the expected training error decreases</li>
      <li>the training variance increases</li>
    </ul>
  </li>
  <li>the variance on future samples increases</li>
</ul>

<p>We don’t know if the whole thing decreases or increases. It is a trade-off.</p>

<h1 id="reference">9. Reference</h1>
<p>[1] Prof. Schuurmans’s notes on Bias-Variance </p>

</div>

          <div class="footer">
            <div class="contact">
              <p>
                My Github<br />
                E-mail<br />
                LinkedIn
              </p>
            </div>
            <div class="contact">
              <p>
                <a href="https://github.com/kinslover">github.com/kinslover</a><br />
                pjin1[[at]]ualberta[dot]ca<br />
                http://www.linkedin.com/pub/ping-jin/3a/a3b/97a
              </p>
            </div>
          </div>



    <!--disqus-->
    
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'kingdomofking'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
    



    </body>
</html>
