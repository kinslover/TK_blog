<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Notes on Chapter 2 of ESL</title>
        <meta name="viewport" content="width=device-width">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

        <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script> 

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-45812015-1', 'kinslover.github.io');
      ga('send', 'pageview');

    </script>            


    </head>
    <body>

        <div class="site">
          <div class="header">
            <h1 class="title"><a href="/">TK_blog</a></h1>
            <a class="extra" href="/">Home</a>
            <a class="extra" href="/Notes">Blog Posts</a>
            <a class="extra" href="/cv/cv.pdf">CV</a>
          </div>

          <h2>Notes on Chapter 2 of ESL</h2>
<p class="meta">18 Nov 2013</p>

<div class="post">
<h1 id="instroduction">2.1 Instroduction</h1>

<p><strong>Supervised Learning</strong>: predict output values from input values.</p>

<p>Some equivalent terminologies:</p>

<ul>
  <li>Input: predictors, independent variables, features</li>
  <li>Ouput: responses, dependent variables</li>
</ul>

<h1 id="variable-types-and-terminology">2.2 Variable Types and Terminology</h1>

<h2 id="three-variable-types">Three variable types</h2>

<ul>
  <li>Quantitative (continuos)
    <ul>
      <li>ratio defined</li>
      <li>ratio not defined</li>
    </ul>
  </li>
  <li>Qualitative (discrete, categorical, factors)</li>
  <li>Ordered categorical</li>
</ul>

<p>We usually do some coding to the qualitative variables</p>

<ul>
  <li>target: for binary varibles, we use 0-1 or (-1)-(+1) coding</li>
  <li>dummy variables: for a variable with <script type="math/tex">K</script> possible values, we use <script type="math/tex">K</script> binary variables to code it</li>
</ul>

<h2 id="notations">Notations:</h2>
<p>Generally, upper case letters represent the random variables, while lower case ones indicate some observed values, which are regarded as fixed or constant.</p>

<ul>
  <li><script type="math/tex">X</script>: input variable or vector, <script type="math/tex">j_{th}</script> component of which is written as <script type="math/tex">X_j</script>, while <script type="math/tex">i_{th}</script> observed value of <script type="math/tex">X</script> is <script type="math/tex">x_i</script></li>
  <li><script type="math/tex">Y, G</script>: output varible. <script type="math/tex">Y</script> for quantative and <script type="math/tex">G</script> for qualitative </li>
  <li><script type="math/tex">\mathbf{X}</script>: <script type="math/tex">N \times p</script> matrix, training data, consist of <script type="math/tex">x_i^T</script>, <script type="math/tex">i = 1,...N</script>. The <script type="math/tex">j_{th}</script> component is <script type="math/tex">\mathbf{x}_j</script></li>
</ul>

<h2 id="learning-task">learning task</h2>
<p>Given the value of an input vector <script type="math/tex">X</script>, make a good prediction of the output <script type="math/tex">Y/G</script>, denoted by <script type="math/tex">\hat{Y}/\hat{G}</script></p>

<h1 id="least-squares-and-nearest-neighbors">Least Squares and Nearest Neighbors</h1>

<ul>
  <li>Linear model
    <ul>
      <li>huge assumptions about the structure </li>
      <li>stable but maybe not very accurate predictions</li>
    </ul>
  </li>
  <li><script type="math/tex">k</script>-nearest neighbor
    <ul>
      <li>few assumptions, if any, about the structure </li>
      <li>accurate but not stable predictions (sensitive to small changes in training dataset)</li>
    </ul>
  </li>
</ul>

<h2 id="linear-models-and-least-squares">Linear Models and Least Squares</h2>

<h3 id="linear-models">Linear Models</h3>
<p>Generally, we use “hat” over something to represent an estimate of it, such as, using <script type="math/tex">\hat{X}</script> to estimate <script type="math/tex">X</script>.</p>

<p>Given a input vector <script type="math/tex">X^T = (X_1,X_2,...,X_p)</script>, we predict the output <script type="math/tex">Y</script> via the model,</p>

<script type="math/tex; mode=display"> \hat{Y} = \hat{\beta}_0 + \sum^p_{j = 1}X_j \hat{\beta_j}</script>

<p><script type="math/tex">\beta_0</script> is known as intercept or bias. If we include <script type="math/tex">\mathbf{1}</script> as a feature, then </p>

<script type="math/tex; mode=display"> \hat{Y} = X^T\hat{\beta}</script>

<p>if the output <script type="math/tex">Y</script> is scaler, then the dimension of <script type="math/tex">\hat{\beta}</script> is <script type="math/tex">1\times p</script>. Generally, the output can be <script type="math/tex">K</script>-vector, then the dimension of <script type="math/tex">\hat{\beta}</script> should be <script type="math/tex">K \times p</script>.</p>

<p>In the <script type="math/tex">p+1</script> dimensional space, <script type="math/tex">(X, \hat{Y})</script> is a hyperplane. If <script type="math/tex">\mathbf{1}</script> is included, the hyperplane includes the origin and thus is a subspace.</p>

<h3 id="least-squares">Least Squares</h3>

<p>We write <script type="math/tex">\hat{\beta}</script> instead of <script type="math/tex">\beta</script>, because we assume that there is a “perfect” <script type="math/tex">\beta</script>. Our task is to estimate the <script type="math/tex">\beta</script> by fitting the data.</p>

<p>One way to do that is <em>least square fit</em>. Define</p>

<script type="math/tex; mode=display"> RSS(\beta) = \sum_{i = 1}^{N} (y_i - x_i^T \beta) = (\mathbf{X}\beta - \mathbf{y})^T(\mathbf{X}\beta - \mathbf{y}) </script>

<p>Then we do the fit as follows</p>

<script type="math/tex; mode=display"> \hat{\beta} = argmin_{\beta} RSS(\beta). </script>

<p>To get the solution, we take derivative of <script type="math/tex">RSS(\beta)</script> w.r.t. <script type="math/tex">\beta</script> and get</p>

<script type="math/tex; mode=display"> \mathbf{X}^T(\mathbf{y} - \mathbf{X}\beta) </script>

<p>Then we set the derivate to zero, and if <script type="math/tex">X^TX</script> is nonsingular, we have </p>

<script type="math/tex; mode=display"> \hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}</script>

<p>And then we have the fitte value <script type="math/tex">\mathbf{\hat{y}}</script> to be</p>

<script type="math/tex; mode=display"> \mathbf{\hat{y}} = \mathbf{X} \hat{\beta}</script>

<p>So the fitted surface <script type="math/tex">(X, \hat{y})</script> is actually <script type="math/tex">(X, X \hat{\beta})</script>, parameterized by <script type="math/tex">\hat{\beta}</script>.</p>

<h2 id="simulation-experiment">Simulation Experiment</h2>
<p>The input <script type="math/tex">X</script> is a two dimensional vector and the ouput variable <script type="math/tex">G</script> is a binary variable. We make prediction <script type="math/tex">\hat{G}</script> as</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
  \hat{G} = 
  \left\{
    \begin{array}{r@{\;=\;}l}
    1 & \hat{Y} > 0.5\\
    0 & \hat{Y} \leq 0.5
    \end{array}
  \right.
\end{equation}
 %]]></script>

<table>
  <tbody>
    <tr>
      <td>where <script type="math/tex">\hat{Y} = X \beta</script> and the <em>decision boundary</em> is $${x</td>
      <td>x\beta = 0.5}$$.</td>
    </tr>
  </tbody>
</table>

<p>Let us consider about how the data is generated</p>

<ul>
  <li><strong>Scenario 1</strong>: The training data in each class were generated from bivariate Gaussian distributions with uncorrelated components and different means, i.e. several independent bivariate Gaussian distributions.</li>
</ul>

<p>[put a figure here]</p>

<ul>
  <li><strong>Scenario 2</strong>: We have ten low-variance Gaussian distribution for each class respectively. Their means are distributed as Gaussian too. The way to generate a sample is to first select one Gaussian distribution out of the ten and then draw a point from that distribution.</li>
</ul>

<p>For scenario 1, a linear boundary is the best we can do. For scenario 2, a nonlinear one is better.</p>

<h2 id="nearest-neighbor-methods">2.3.2 Nearest-Neighbor Methods</h2>

<script type="math/tex; mode=display">
\hat{Y}(X) =\frac{1}{k} \sum_{x_i \in N_k(X)}(y_i)
</script>

<p>where <script type="math/tex">N_k(X)</script> is the set of <script type="math/tex">k</script> training samples that are nearest to <script type="math/tex">X</script>.</p>

<table>
  <tbody>
    <tr>
      <td>If the output variable is categorical, we can still put a threshold (decision boundary) on <script type="math/tex">\hat{Y}</script> to decide <script type="math/tex">\hat{G}</script>. A majortiy vote is the case where the decision boundary is $${x</td>
      <td>\hat{Y}(x) = 0.5}<script type="math/tex">. Usually the decision boundary of </script>k$$NN is irregular.</td>
    </tr>
  </tbody>
</table>

<p>Though it seems we only have one parameter <script type="math/tex">k</script> in <script type="math/tex">k</script>NN instead of <script type="math/tex">p</script> in Linear model. Actually the effective number of parameter in <script type="math/tex">k</script>NN is <script type="math/tex">\frac{N}{k}</script>, which is generally greater than <script type="math/tex">p</script>.</p>

<h2 id="from-least-squares-to-nearest-neighbors">2.3.3 From Least Squares to Nearest Neighbors</h2>

<p>Least Squares method has stronly replied the linear assumption, which makes it a simple model. This simplicity makes it less sensitive to the change of training dataset and thus enjoy a low variance, but potentially a high bias. For <script type="math/tex">k</script>NN, it hardly relies on any strict assumption, which makes it enjoy a low bias, but a high variance, i.e. quite sensitive to the change of training set (for small <script type="math/tex">k</script>).</p>

<p>For senario 1, linear model is more appropriate, while <script type="math/tex">k</script>NN works better for senario 2.</p>

<p>Many most popular techniques come from these two simple procedure. Below list some popular ways of enhancing these two methods:</p>

<blockquote>
  <ul>
    <li>Kernel methods use <strong>weights that decrease smoothly to zero with distance</strong> from the target point, rather than the effective 0/1 weights used by k-nearest neighbors.</li>
    <li>In high-dimensional spaces the distance kernels are modified to <strong>emphasize some variable</strong> more than others.</li>
    <li>Local regression fits linear models by <strong>locally weighted least squares</strong>, rather than fitting constants locally.</li>
    <li>Linear models fit to a <strong>basis expansion</strong> of the original inputs allow arbitrarily complex models.</li>
    <li>Projection pursuit and neural network models consist of sums of <strong>non-linearly transformed</strong> linear models.</li>
  </ul>
</blockquote>

<h1 id="statistical-decision-theory">2.4 Statistical Decision Theory</h1>

<p>Let <script type="math/tex">X \in \mathbb{R}^p</script> be the input vector, with real valued variables and <script type="math/tex">Y\in \mathbb{R}</script>. And the underlied joint distribution is <script type="math/tex">Pr(X, Y)</script>. Then suppose that our loss function is <script type="math/tex">L(Y, f(X)) = (Y - f(x))^2</script>, i.e. the squared loss. Then we have a measure for function <script type="math/tex">f</script>, which we can use as a criterion to selecte <script type="math/tex">f</script>,</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
  \begin{split}
    EPE(f) &= E(Y - f(X))^2\\
    &= \int [y - f(x)]^2 Pr(dx, dy)
  \end{split}
\end{equation}
 %]]></script>

<p>Then we can condition it on <script type="math/tex">X</script></p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
  \begin{split}
    EPE(f) &= E_{X}E_{Y|X}[(Y - f(X))^2|X]\\
  \end{split}
\end{equation}
 %]]></script>

<p>By minimizing it pointwisely, we get the solution</p>

<script type="math/tex; mode=display">
f(x) = E(Y| X = x)
</script>

<p>This is also known as the <em>regression function</em>. It is the best prediction of <script type="math/tex">Y</script> at any point <script type="math/tex">X = x</script>, given the perfect knowledge of <script type="math/tex">Pr(X, Y)</script>, w.r.t. the averaged squared error.</p>

<table>
  <tbody>
    <tr>
      <td>Note if we change the loss function, the solution is different. For example, if we use $$L(Y) =</td>
      <td>Y-f(x)</td>
      <td>$$, the solution will be the conditional median instead of the conditional mean. This is actually more robust than least squares, as it is much less sensitive to the outlier than least square fit. However, as the derivate of it is not continuos at zero, which makes it hard to optimize and lose the closed form solution.</td>
    </tr>
  </tbody>
</table>

<h3 id="nearest-neighbor">Nearest-Neighbor</h3>
<p>The nearest-neighbor methods exactly follow this thought in an approximated way,</p>

<script type="math/tex; mode=display">
\hat{f}(x) = Ave(y_i| x_i \in N_k(x)),
</script>

<p>And the two approximations used here are</p>

<blockquote>
  <ul>
    <li>expectation is approximated by averaging over <strong>sample data</strong>;</li>
    <li>conditioning at a point is <strong>relaxed</strong> to conditioning on some region
“close” to the target point.</li>
  </ul>
</blockquote>

<table>
  <tbody>
    <tr>
      <td>It is easy to see that as <script type="math/tex">N,K \to \infty, s.t. k/N \to 0</script>, we have $$\hat{f}(x) \to E[Y</td>
      <td>X = x]<script type="math/tex">. But the problem is that we usually don't have that much data, i.e the first approximation is bad. Another problem is that as </script>p$$ increases, the data samples we have will be scattered very sparsely in the space, which leads to fail of using nearest neighbor as a surrorgate for conditioning, i.e. the second approximation is not good. And the <em>convergence rate</em> also decreases.</td>
    </tr>
  </tbody>
</table>

<h3 id="linear-regression">Linear regression</h3>

<p>Linear regression is a <em>model-based</em> approach. It means that we have the belief/assumption that the regression function is approximately linear in its arguments:</p>

<script type="math/tex; mode=display">
f(x) = E(Y| X = x) \approx x^T\beta
</script>

<p>If this assumption is satisfied, we have </p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{equation}
  \begin{split}
    EPE(f) &= E[(Y - X^T\beta)^2]\\
  \end{split}
\end{equation}
 %]]></script>

<p>Then by differentiating it w.r.t <script type="math/tex">\beta</script> and set the gradient to zero, we can get the closed form solution</p>

<script type="math/tex; mode=display">
\beta = [E(XX^T)]^{-1}E(XY)
</script>

<p>Thus actually both nearest-neighbor and linear model are trying to approximate the condtional expectations by averages.</p>

<h1 id="local-methods-in-high-dimensions">2.5 Local Methods in High Dimensions</h1>


</div>

          <div class="footer">
            <div class="contact">
              <p>
                My Github<br />
                E-mail<br />
                LinkedIn
              </p>
            </div>
            <div class="contact">
              <p>
                <a href="https://github.com/kinslover">github.com/kinslover</a><br />
                pjin1[[at]]ualberta[dot]ca<br />
                http://www.linkedin.com/pub/ping-jin/3a/a3b/97a
              </p>
            </div>
          </div>



    <!--disqus-->
    
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'kingdomofking'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
    



    </body>
</html>
