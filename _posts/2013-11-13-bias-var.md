---
layout: post
title: Bias-Variance
comments: true
published: true

---

## Assumptions


All examples are $$i.i.d$$, from the joint distribution $$P(X, Y)$$. In this article, uppercase letters represent the random variables, while lowercase letters represent constant values.


## Three levels

- **Prediction**:Given a dataset $$d$$, the hypothesis learned from this dataset, and the value of $$x$$. The prediction $$h(x)$$ can be evaluated by the error . 

$$
E_{Y}[err(h(x|d), Y)]
$$

- **Hypothesis**: maps *X* to *Y*. It can be evaluated by the quality of predictions. It means that we want to measure the expected error of the hypothesis learned from some training set $$d$$ over the distribution $$P(X, Y)$$.

$$
E_{X,Y}[err(h(X|d), Y)]
$$

- **Learner**: maps *dataset D* to *hypothesis h*. It can be evaluated by the quality of hypothesis. It means that we want to measure the expectation of the performance of the hypotheses over different training datasets $$D$$.

$$
E_{D}[E_{X,Y}[err(h(X|D), Y)]]
$$


## Error function

So let us focus on the case with $$L2$$ error function. 

$$\begin{equation}
	err(h(x|d), Y) = (h(x|d) - Y)^2
\end{equation}$$

## Analysis for evaluation of predictions

Thus for a given $$x$$ and $$d$$, we measure our prediction over the distribution $$P(Y|x)$$


$$\begin{equation}
	\begin{split}
		 E_{Y|x}[err(h(x|d), Y)] &= E_{Y|x}[(h(x|d) - Y)^2]\\
		&= E_{Y|x}[(h(x|d) -E[Y|x]+E[Y|x] - Y)^2]\\
		&= E_{Y|x}[(h(x|d) -E[Y|x])^2] + E_{Y|x}[(E[Y|x] - Y)^2] \\
		&+ 2 E_{Y|x}[(h(X|d)-E[Y|x])(E[Y|x] - Y)]\\
		&= (h(x|d) -E[Y|x])^2 + E_{Y|x}[(E[Y|x] - Y)^2] \\		
	\end{split}
\end{equation}$$

The fisrt term in this equation is the square of the bias and the second term is the variance of the variable $$Y|x$$, which is a property of the distribution and thus totally out of our control.

And from the above derivation, we know that we should set 

$$
h(x|d) = E[Y|x]
$$

to minimize the expected error of our prediction.

(to be continued)